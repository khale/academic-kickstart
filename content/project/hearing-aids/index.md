---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "Enabling Next-Generation AI Hearing Aids"
summary: ""
authors: ["Prem Seetharaman", "Bryan Pardo", "Rujia Wang", "Pamela Souza", "Hussain Khajanchi", "Iris Uwizeyimana", admin]
tags: [os, arch, ai, hearing]
categories: []
date: 2020-03-16T09:38:39-05:00

# Optional external URL for project (replaces project detail page).
external_link: ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: "" 
  preview_only: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""
---

Although the loss of audibility associated with age-related hearing loss is
relatively easy to address via ap- propriate frequency-gain amplification used
in today's hearing aids, difficulty hearing in noisy environments is not.
Existing algorithms deployed in hearing aids cannot focus on the sound source
of interest when similar sounds occur in similar proximity and/or intensity
(e.g. focusing on your dinner companion over diners at a neighboring restaurant
table). Audio source separation technology is designed to isolate a specific
source in a complex audio scene, like a cocktail party. In this project, we are 
developing
and deploying cutting-edge audio source separation on hearing-assistive devices,
with the goal of amplifying only the desired sound in a complex scene. We are
creating new machine learning algorithms and optimizations designed for real-time
audio processing in resource-constrained computing environments like
smartphones and hearing aids. We will develop new methods to let end-users
personalize deep source separation models to their hearing and health needs. We
will create system designs, including innovative hardware prototypes and
interactive software interfaces to support a broad set of audio-centric machine
learning computing scenarios. 

This project is in collaboration with [Rujia Wang](https://rujiawang.github.io/)
at IIT, and [Pam Souza](https://www.communication.northwestern.edu/faculty/PamelaSouza), [Bryan Pardo](https://users.cs.northwestern.edu/~pardo/), and [Prem Seetharaman](https://pseeth.github.io/) at
Northwestern University.

